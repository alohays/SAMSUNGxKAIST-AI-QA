{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ALBERT_Model_Assign.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jeNg-87Z5G7",
        "outputId": "84fd567c-0434-46a5-8180-d583ef38b439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WuxHgPYnpQYp",
        "outputId": "dbf97012-8048-44dd-a01a-e9d185f839a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd drive/My Drive/DAVIAN_SharedFolder/SAMSUNG_KAIST_lectures/191017_SamsungKAIST_QA/assignment"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DAVIAN_SharedFolder/SAMSUNG_KAIST_lectures/191017_SamsungKAIST_QA/assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1fyE9FLMjP",
        "colab_type": "code",
        "outputId": "9cd67011-9f09-4d0f-9cdf-005052525ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02_BERT_SQuAD_assign_answer.ipynb  02_BERT_SQuAD_assign.ipynb  TrainedModel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0wpld8GYto7",
        "colab_type": "text"
      },
      "source": [
        "# ALBERT Model\n",
        "\n",
        "BERT model과는 다른 ALBERT model의 Cross-Layer Parameter Sharing과 Factorized Embedding Parameterziation을 완성하십시오\n",
        "\n",
        "- Cross-Layer Parameter Sharing : ALBERT use cross-layer parameter sharing in Attention and FFN(FeedForward Network) to reduce number of parameter.\n",
        "- Factorized Embedding Parameterziation : ALBERT seperated Embedding matrix(VxD) to VxE and ExD\n",
        "\n",
        "\n",
        "original BERT code 는 [여기](https://github.com/dhlee347/pytorchic-bert)를 참고하십시오 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24QtkkzcZH1D",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSdSt03XZDQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import math\n",
        "import json\n",
        "from typing import NamedTuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "def split_last(x, shape):\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d0W-Hm6Zcrv",
        "colab_type": "text"
      },
      "source": [
        "## ALBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRXcO3aiQnKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(NamedTuple):\n",
        "    \"Configuration for BERT model\"\n",
        "    vocab_size: int = None # Size of Vocabulary\n",
        "    hidden: int = 768 # Dimension of Hidden Layer in Transformer Encoder\n",
        "    hidden_ff: int = 768*4 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n",
        "    embedding: int = 128 # Factorized embedding parameterization\n",
        "\n",
        "    n_layers: int = 12 # Numher of Hidden Layers\n",
        "    n_heads: int = 768//64 # Numher of Heads in Multi-Headed Attention Layers\n",
        "    #activ_fn: str = \"gelu\" # Non-linear Activation Function Type in Hidden Layers\n",
        "    max_len: int = 512 # Maximum Length for Positional Embeddings\n",
        "    n_segments: int = 2 # Number of Sentence Segments\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, file):\n",
        "        return cls(**json.load(open(file, \"r\")))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"Implementation of the gelu activation function by Hugging Face\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"A layernorm module in the TF style (epsilon inside the square root).\"\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.hidden))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.hidden))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsAQqb3cSef",
        "colab_type": "text"
      },
      "source": [
        "### Factorized Embedding Parameterziation\n",
        "\n",
        "??? 부분을 완성하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yB-_owMcN77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # Original BERT Embedding\n",
        "        # self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.hidden) # token embedding\n",
        "\n",
        "        # factorized embedding\n",
        "        self.tok_embed1 = nn.Embedding(cfg.????, cfg.????)\n",
        "        self.tok_embed2 = nn.Linear(cfg.????, cfg.????)\n",
        "\n",
        "        self.pos_embed = nn.Embedding(???????????, cfg.hidden) # position embedding\n",
        "        self.seg_embed = nn.Embedding(???????????, cfg.hidden) # segment(token type) embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        # self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        # factorized embedding\n",
        "        e = ????????????(x)\n",
        "        e = ????????????(e)\n",
        "        e = e + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        #return self.drop(self.norm(e))\n",
        "        return self.norm(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aREvl6O7bXoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "        self.proj_k = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "        self.proj_v = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "        # self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "        #scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = (scores @ v).transpose(1, 2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.hidden, cfg.hidden_ff)\n",
        "        self.fc2 = nn.Linear(cfg.hidden_ff, cfg.hidden)\n",
        "        #self.activ = lambda x: activ_fn(cfg.activ_fn, x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "# original BERT code\n",
        "# class Block(nn.Module):\n",
        "#     \"\"\" Transformer Block \"\"\"\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.attn = MultiHeadedSelfAttention(cfg)\n",
        "#         self.proj = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "#         self.norm1 = LayerNorm(cfg)\n",
        "#         self.pwff = PositionWiseFeedForward(cfg)\n",
        "#         self.norm2 = LayerNorm(cfg)\n",
        "#         self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "#\n",
        "#     def forward(self, x, mask):\n",
        "#         h = self.attn(x, mask)\n",
        "#         h = self.norm1(x + self.drop(self.proj(h)))\n",
        "#         h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "#         return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No8osBXScfM1",
        "colab_type": "text"
      },
      "source": [
        "### Cross-Layer Parameter Sharing \n",
        "\n",
        "??? 부분을 완성하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_rluP-OW2Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.embed = Embeddings(cfg)\n",
        "        # Original BERT not used parameter-sharing strategies\n",
        "        # self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        # To used parameter-sharing strategies\n",
        "        self.n_layers = ???????????\n",
        "        self.attn = MultiHeadedSelfAttention(cfg)\n",
        "        self.proj = nn.??????(cfg.hidden, ?????????)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = ????????????????????????????\n",
        "        self.norm2 = ??????????????\n",
        "        # self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, seg, mask):\n",
        "        h = self.embed(x, seg)\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            # h = block(h, mask)\n",
        "            h = self.attn(h, mask)\n",
        "            h = self.norm1(h + ?????????)\n",
        "            h = self.norm2(h + ?????????)\n",
        "\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}